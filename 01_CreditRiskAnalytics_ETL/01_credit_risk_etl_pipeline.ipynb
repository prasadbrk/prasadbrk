{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37202e83-9c6c-4763-b27e-c738d8efd601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Credit Risk Analytics ETL Pipeline\n",
    "# Production-Grade Banking Data Engineering Solution\n",
    "# ============================================================================\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, lit, concat, upper, lower, substring, length,\n",
    "    to_date, current_timestamp, year, month, dayofmonth,\n",
    "    count, sum as spark_sum, avg, max as spark_max, min as spark_min,\n",
    "    coalesce, row_number\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Starting Credit Risk Analytics ETL Pipeline\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAGE 1: EXTRACT - Load raw data from sources\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== STAGE 1: EXTRACT ===\")\n",
    "\n",
    "# Create sample customer data for demonstration\n",
    "customer_data = [\n",
    "    (1, \"John Doe\", \"john.doe@bank.com\", \"2015-01-15\", \"Active\", \"5000000\"),\n",
    "    (2, \"Jane Smith\", \"jane.smith@bank.com\", \"2018-03-22\", \"Active\", \"7500000\"),\n",
    "    (3, \"Bob Johnson\", \"bob.johnson@bank.com\", \"2019-06-10\", \"Inactive\", \"3000000\"),\n",
    "    (4, \"Alice Williams\", \"alice.williams@bank.com\", \"2017-11-05\", \"Active\", \"9000000\"),\n",
    "    (5, \"Charlie Brown\", \"charlie.brown@bank.com\", \"2016-09-18\", \"Active\", \"6500000\"),\n",
    "]\n",
    "\n",
    "customer_df = spark.createDataFrame(\n",
    "    customer_data,\n",
    "    [\"customer_id\", \"customer_name\", \"email\", \"account_open_date\", \"status\", \"annual_income\"]\n",
    ")\n",
    "\n",
    "logger.info(f\"Extracted {customer_df.count()} customer records\")\n",
    "print(f\"Extracted {customer_df.count()} customer records\")\n",
    "\n",
    "# Create sample loan data\n",
    "loan_data = [\n",
    "    (1, 1, 500000, 9.5, \"2023-01-15\", \"Active\", 60, 8500, \"Home Loan\"),\n",
    "    (2, 2, 750000, 8.75, \"2023-02-20\", \"Active\", 84, 10200, \"Home Loan\"),\n",
    "    (3, 3, 300000, 12.5, \"2023-03-10\", \"Defaulted\", 36, 9800, \"Personal Loan\"),\n",
    "    (4, 4, 900000, 7.99, \"2023-04-05\", \"Active\", 120, 8100, \"Home Loan\"),\n",
    "    (5, 5, 650000, 9.99, \"2023-05-12\", \"Active\", 84, 9500, \"Home Loan\"),\n",
    "]\n",
    "\n",
    "loan_df = spark.createDataFrame(\n",
    "    loan_data,\n",
    "    [\"loan_id\", \"customer_id\", \"loan_amount\", \"interest_rate\", \"disbursement_date\", \n",
    "     \"loan_status\", \"tenure_months\", \"emi_amount\", \"loan_type\"]\n",
    ")\n",
    "\n",
    "logger.info(f\"Extracted {loan_df.count()} loan records\")\n",
    "print(f\"Extracted {loan_df.count()} loan records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0ce62d6-216a-4594-802a-f3c4ea0c2d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAGE 2: TRANSFORM - Data transformation and enrichment\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== STAGE 2: TRANSFORM ===\")\n",
    "\n",
    "# Data Quality Checks\n",
    "logger.info(\"Performing data quality checks...\")\n",
    "\n",
    "# Convert data types\n",
    "customer_df = customer_df.withColumn(\"account_open_date\", to_date(col(\"account_open_date\"), \"yyyy-MM-dd\")) \\\n",
    "                        .withColumn(\"annual_income\", col(\"annual_income\").cast(\"long\"))\n",
    "\n",
    "loan_df = loan_df.withColumn(\"disbursement_date\", to_date(col(\"disbursement_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "logger.info(\"Data type conversion completed\")\n",
    "\n",
    "# Join customer and loan data\n",
    "merged_df = loan_df.join(customer_df, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "logger.info(f\"Merged customer and loan data: {merged_df.count()} records\")\n",
    "\n",
    "# Calculate credit risk metrics\n",
    "risk_df = merged_df.withColumn(\n",
    "    \"loan_to_income_ratio\",\n",
    "    col(\"loan_amount\") / col(\"annual_income\")\n",
    ").withColumn(\n",
    "    \"annual_emi\",\n",
    "    col(\"emi_amount\") * 12\n",
    ").withColumn(\n",
    "    \"emi_to_income_ratio\",\n",
    "    col(\"annual_emi\") / col(\"annual_income\")\n",
    ").withColumn(\n",
    "    \"risk_category\",\n",
    "    when(col(\"loan_status\") == \"Defaulted\", \"High\")\n",
    "    .when((col(\"emi_to_income_ratio\") > 0.4) | (col(\"loan_to_income_ratio\") > 0.8), \"High\")\n",
    "    .when((col(\"emi_to_income_ratio\") > 0.25) | (col(\"loan_to_income_ratio\") > 0.5), \"Medium\")\n",
    "    .otherwise(\"Low\")\n",
    ").withColumn(\n",
    "    \"processing_date\",\n",
    "    current_timestamp()\n",
    ")\n",
    "\n",
    "logger.info(\"Risk metrics calculated successfully\")\n",
    "print(f\"Risk calculation completed for {risk_df.count()} records\")\n",
    "\n",
    "# Display sample transformed data\n",
    "print(\"\\nSample Transformed Data:\")\n",
    "risk_df.select(\n",
    "    \"customer_id\", \"loan_id\", \"loan_amount\", \"annual_income\", \n",
    "    \"loan_to_income_ratio\", \"risk_category\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc7b267a-eeef-4f73-a12c-bec88b6d0784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAGE 3: LOAD - Write aggregated data to target systems\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== STAGE 3: LOAD ===\")\n",
    "\n",
    "# Create aggregation tables\n",
    "print(\"\\nCreating aggregation tables...\")\n",
    "\n",
    "# Portfolio summary by risk category\n",
    "portfolio_summary = risk_df.groupBy(\"risk_category\").agg(\n",
    "    count(\"*\").alias(\"num_loans\"),\n",
    "    spark_sum(\"loan_amount\").alias(\"total_loan_amount\"),\n",
    "    avg(\"loan_to_income_ratio\").alias(\"avg_loan_to_income_ratio\"),\n",
    "    spark_sum(col(\"loan_amount\") * col(\"loan_to_income_ratio\")).alias(\"risk_weighted_amount\")\n",
    ").withColumn(\"processing_date\", current_timestamp())\n",
    "\n",
    "logger.info(\"Portfolio summary created\")\n",
    "print(\"\\nPortfolio Summary by Risk Category:\")\n",
    "portfolio_summary.show()\n",
    "\n",
    "# Default risk analysis\n",
    "default_analysis = risk_df.filter(col(\"loan_status\") == \"Defaulted\").groupBy(\"risk_category\").agg(\n",
    "    count(\"*\").alias(\"defaulted_loans\"),\n",
    "    spark_sum(\"loan_amount\").alias(\"defaulted_amount\"),\n",
    "    avg(\"emi_to_income_ratio\").alias(\"avg_emi_ratio\")\n",
    ")\n",
    "\n",
    "logger.info(\"Default risk analysis completed\")\n",
    "print(\"\\nDefault Risk Analysis:\")\n",
    "default_analysis.show()\n",
    "\n",
    "# Customer level analytics\n",
    "customer_analytics = risk_df.groupBy(\"customer_id\", \"customer_name\", \"status\").agg(\n",
    "    count(\"*\").alias(\"num_loans\"),\n",
    "    spark_sum(\"loan_amount\").alias(\"total_loan_exposure\"),\n",
    "    avg(\"loan_to_income_ratio\").alias(\"avg_lti_ratio\"),\n",
    "    spark_max(\"risk_category\").alias(\"max_risk_category\")\n",
    ").withColumn(\"processing_date\", current_timestamp())\n",
    "\n",
    "logger.info(\"Customer analytics created\")\n",
    "print(\"\\nCustomer Analytics:\")\n",
    "customer_analytics.show()\n",
    "\n",
    "# Quality Metrics\n",
    "print(\"\\n=== DATA QUALITY METRICS ===\")\n",
    "print(f\"Total Records Processed: {risk_df.count()}\")\n",
    "print(f\"High Risk Loans: {risk_df.filter(col('risk_category') == 'High').count()}\")\n",
    "print(f\"Medium Risk Loans: {risk_df.filter(col('risk_category') == 'Medium').count()}\")\n",
    "print(f\"Low Risk Loans: {risk_df.filter(col('risk_category') == 'Low').count()}\")\n",
    "print(f\"Pipeline Status: SUCCESS\")\n",
    "\n",
    "logger.info(\"ETL Pipeline completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "078b86bc-e287-40bc-831a-c94e422aa625",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_credit_risk_etl_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

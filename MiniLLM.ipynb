{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4710bbba-11cb-4d4c-9c00-98333e0d0b38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install langchain langchain-text-splitters sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31631d02-11cb-4f57-8cb7-4e6a5484f3c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# If using local Jupyter, uncomment the next line. In Databricks, 'spark' exists automatically.\n",
    "spark = SparkSession.builder.appName(\"RAG_Project\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (101, \"Commercial Credit Policy\", \"Section 404: LTV Limits. For Commercial Real Estate (CRE), the maximum Loan-to-Value (LTV) ratio is 75%. Exception: If the property is owner-occupied, LTV can go up to 80% with CRO approval.\"),\n",
    "    (102, \"Agri-Loan Policy\", \"Section 202: Seasonal Crops. Loans for seasonal crops must be repaid within 12 months. High-risk crops (e.g., chili, vanilla) require crop insurance mandatory for limits above $50k.\"),\n",
    "    (103, \"Retail Housing Policy\", \"Section 105: Income Verification. For salaried employees, Form 16 is mandatory. For self-employed, last 3 years ITR is required. Minimum credit score for unsecured loans is 750.\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"doc_id\", IntegerType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"full_text\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_bronze = spark.createDataFrame(data, schema)\n",
    "display(df_bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62d0f88e-3bca-4bf1-80f2-5eb6b1e7c915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, explode, col\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. Define the splitting logic (Python function)\n",
    "def splitter_func(text):\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # \"Chunk Size 50\" is small for demo; in production use 500-1000\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=50, \n",
    "        chunk_overlap=10,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "# 2. Register as a Spark UDF (User Defined Function)\n",
    "# We return an Array of Strings because one document becomes MANY chunks\n",
    "chunk_udf = udf(splitter_func, ArrayType(StringType()))\n",
    "\n",
    "# 3. Apply UDF and \"Explode\" (Flatten) the results\n",
    "df_silver_chunks = df_bronze.withColumn(\"chunk\", explode(chunk_udf(col(\"full_text\")))) \\\n",
    "                            .select(\"doc_id\", \"title\", \"chunk\")\n",
    "\n",
    "display(df_silver_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d8a0f61-0f36-438e-a415-c560fa5b764a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver_chunks.show(vertical=True, truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20e0b461-ac62-4d6c-9a07-5d17f2359418",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Convert our Spark Chunked Data to Pandas (for local model processing)\n",
    "# In production with 1B rows, you wouldn't do this! You'd use mapPartitions.\n",
    "pdf = df_silver_chunks.toPandas()\n",
    "\n",
    "# 2. Load the \"Brain\" (The Embedding Model)\n",
    "# This downloads a small model (~80MB) that turns text into 384 numbers.\n",
    "print(\"Loading model... this might take a minute...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 3. Generate Vectors\n",
    "# We apply the model to every text chunk.\n",
    "print(\"Generating vectors...\")\n",
    "pdf['vector'] = pdf['chunk'].apply(lambda x: model.encode(x))\n",
    "\n",
    "# 4. Display the \"Magic\"\n",
    "# Look at the 'vector' column. It's not text anymore; it's a list of numbers.\n",
    "print(\"Done!\")\n",
    "pdf.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b32967dd-6662-412e-a5d5-1c343f528519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# 1. Prepare Data for FAISS\n",
    "# FAISS expects a Matrix of Float32 numbers.\n",
    "# We stack our list of vectors into a single numpy array.\n",
    "embeddings_matrix = np.stack(pdf['vector'].values).astype('float32')\n",
    "\n",
    "# 2. Create the Index\n",
    "# \"d\" is the dimension of the vector (384 for MiniLM)\n",
    "d = embeddings_matrix.shape[1]\n",
    "index = faiss.IndexFlatL2(d)  # L2 = Euclidean Distance (standard for simple search)\n",
    "\n",
    "# 3. Add vectors to the index\n",
    "index.add(embeddings_matrix)\n",
    "print(f\"Number of documents in index: {index.ntotal}\")\n",
    "\n",
    "# --- THE MOMENT OF TRUTH ---\n",
    "\n",
    "# 4. Define a User Question\n",
    "query_text = \"What is the max LTV for commercial real estate?\"\n",
    "\n",
    "# 5. Convert Question to Vector (using the same brain/model)\n",
    "query_vector = model.encode([query_text]).astype('float32')\n",
    "\n",
    "# 6. Search the Index\n",
    "# k=2 means \"Give me the top 2 closest matches\"\n",
    "distances, indices = index.search(query_vector, k=2)\n",
    "\n",
    "# 7. Display Results\n",
    "print(f\"\\nQuery: '{query_text}'\\n\")\n",
    "print(\"--- Top Retrieved Results ---\")\n",
    "\n",
    "for i, doc_index in enumerate(indices[0]):\n",
    "    # Get the row from our original Pandas dataframe using the index found by FAISS\n",
    "    result_row = pdf.iloc[doc_index]\n",
    "    print(f\"Result {i+1} (Score: {distances[0][i]:.4f}):\")\n",
    "    print(f\"Doc Title: {result_row['title']}\")\n",
    "    print(f\"Text Chunk: {result_row['chunk']}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MiniLLM",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

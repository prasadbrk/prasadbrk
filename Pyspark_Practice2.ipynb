{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93eac5b5-765c-4506-89bb-0f6b40af7e8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"sales\").getOrCreate()\n",
    "schema = [\"customer_id\",\"join_date\",\"region\"]\n",
    "data = [(\"C1001\",\"2023-01-05\",\"East\"),\n",
    "(\"C1002\",\"2023-03-12\",\"West\"),\n",
    "(\"C1003\",\"2023-07-25\",\"East\"),\n",
    "(\"C1004\",\"2024-02-01\",\"West\"),\n",
    "(\"C1005\",\"2024-05-30\",\"Central\")]\n",
    "cust=spark.createDataFrame(data,schema)\n",
    "cust.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1c8c4ba-8f6b-41bd-83df-ae511096db98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Create SparkSession (if not already running)\n",
    "spark = SparkSession.builder.appName(\"MockDataCreation\").getOrCreate()\n",
    "\n",
    "# 2. Define the raw data as a list of tuples\n",
    "raw_sales_data = [\n",
    "    (\"T100\", \"C1001\", \"50.00\", \"2024-01-10 10:00:00\"),\n",
    "    (\"T101\", \"C1002\", \"125.50\", \"2024-01-15 14:30:00\"),\n",
    "    (\"T102\", \"C1001\", \"20.00\", \"2024-02-01 09:15:00\"),\n",
    "    (\"T103\", \"C1003\", \"300.00\", \"2024-03-20 18:45:00\"),\n",
    "    (\"T104\", \"C1002\", \"75.00\", \"2024-04-05 11:20:00\"),\n",
    "    (\"T105\", \"C1001\", \"15.00\", \"2024-09-01 16:00:00\"),\n",
    "    (\"T106\", \"C1004\", \"5.50\", \"2024-09-05 08:30:00\"),\n",
    "    (\"T107\", \"C1003\", \"450.00\", \"2024-09-10 12:00:00\"),\n",
    "    (\"T108\", \"C1005\", \"1000.00\", \"2024-10-01 13:10:00\"),\n",
    "    (\"T109\", \"C1001\", \"10.00\", \"2024-10-10 17:00:00\"),\n",
    "    (\"T110\", \"C1002\", \"20.00\", \"2024-10-14 09:00:00\")\n",
    "]\n",
    "\n",
    "# 3. Define the desired schema, explicitly casting 'amount' and 'timestamp'\n",
    "# This ensures data types are correct for later calculations.\n",
    "sales_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), False),\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"amount\", DoubleType(), True),         # Correctly cast to Double\n",
    "    StructField(\"timestamp\", TimestampType(), True)    # Correctly cast to Timestamp\n",
    "])\n",
    "\n",
    "# 4. Create the DataFrame\n",
    "# We use F.to_timestamp to convert the string date to TimestampType during creation\n",
    "sales_df = spark.createDataFrame(\n",
    "    raw_sales_data,\n",
    "    ['transaction_id', 'customer_id', 'amount_str', 'timestamp_str'] # Use temporary string column names\n",
    ").withColumn(\"amount\", F.col(\"amount_str\").cast(DoubleType())) \\\n",
    " .withColumn(\"timestamp\", F.to_timestamp(F.col(\"timestamp_str\"))) \\\n",
    " .select(\"transaction_id\", \"customer_id\", \"amount\", \"timestamp\") # Select final, correctly cast columns\n",
    "\n",
    "# 5. Show the resulting DataFrame and its schema\n",
    "print(\"Mock Sales DataFrame:\")\n",
    "sales_df.show()\n",
    "print(\"Schema:\")\n",
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d78bb210-5d62-4d0b-8180-a11cc5f833f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "Today = f.to_date(f.lit(\"2025-10-14\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d575931-cd3b-42ef-95cd-9d087d70b8fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_metrics_df = sales_df.groupBy(\"customer_id\").agg(f.sum(\"amount\").alias(\"Total_sales\"),f.countDistinct(\"transaction_id\").alias(\"Total_orders\"))\n",
    "customer_metrics_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b8be44d-1292-4492-aa5c-a2dcab39e6b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_metrics_df=customer_metrics_df.withColumn(\"avg_tkt_value\",f.col(\"Total_sales\")/f.col(\"Total_orders\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17d1f816-6db6-4fd7-aaa5-6b7fc50bdf6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_metrics_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a47f61-1ec5-423c-84a7-6b91a436bba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "recency_df = sales_df.groupBy(\"customer_id\").agg(f.max(\"timestamp\").alias(\"last_purchase\"))\n",
    "recency_df.display()\n",
    "recency_df=recency_df.withColumn(\"recency\",f.datediff(Today,f.col(\"last_purchase\")))\n",
    "recency_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4840baa-0a53-485d-bb8e-00827cd4dda5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_metrics_df = customer_metrics_df.join(recency_df, on=\"customer_id\", how=\"inner\")\n",
    "final_metrics_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b5ec1ee-9a66-4806-91a0-589db8e7174f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_final_df = final_metrics_df.join(cust, on=\"customer_id\", how=\"left\")\n",
    "customer_final_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc167dbf-577b-4c34-8ab9-d647406042a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "window_spec = Window.partitionBy(\"region\").orderBy(f.desc(\"Total_sales\"))\n",
    "ranked_df = customer_final_df.withColumn(\"rank\",f.rank().over(window_spec))\n",
    "ranked_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab10add9-247b-4e9e-b218-06867e12e4ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "segmented_df = ranked_df.withColumn(\"topCustomer\", f.when(f.col(\"rank\") <= 0.1*f.count(\"*\").over(Window.partitionBy(\"region\")), \"Yes\").otherwise(\"No\"))\n",
    "segmented_df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9e0f6f9-ad08-4bf5-b902-39b41cfdfa5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "segmented_df = segmented_df.withColumn(\n",
    "    \"is_top_clv_customer\",\n",
    "    F.when(F.col(\"rank\") <= 0.1 * F.count(\"*\").over(Window.partitionBy(\"region\")), \"Yes\").otherwise(\"No\")\n",
    ")\n",
    "segmented_df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark_Practice2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf32c693-741a-4c4f-a1c0-b3ac1adc3623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27c300a8-633d-40c9-b7d4-b844b18789c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- PART 1: The Setup ðŸ› ï¸ ---\n",
    "# We configure Spark to be ready for some heavy lifting\n",
    "spark = SparkSession.builder.appName(\"CreditRiskIngestion\").getOrCreate()\n",
    "# --- PART 2: The Download (Python Layer) ðŸ ---\n",
    "# URL to a public subset of LendingClub data (hosted on GitHub for easy access)\n",
    "# Note: Real LendingClub data is huge, this is a workable subset for our demo.\n",
    "dataset_url = \"https://raw.githubusercontent.com/anushkaparadkar/lending-club-case-study/master/loan.csv\"\n",
    "local_file_name = \"/Volumes/workspace/azure_data/az/lending_club_loans.csv\"\n",
    "\n",
    "print(f\"Downloading dataset from {dataset_url}\")\n",
    "# We use urllib to fetch the file to our local environment\n",
    "try:\n",
    "    if not os.path.exists(local_file_name):\n",
    "        urllib.request.urlretrieve(dataset_url,local_file_name)\n",
    "        print(\"Download complete\")\n",
    "    else:\n",
    "        print(\"File already exists, skipping download\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "243aab13-884b-4e95-9d0f-fc3a45cc58e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Load Data into PySpark DataFrame\n",
    "# inferSchema=True is useful but can be slow on huge data; fine for this size (~40MB)\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(local_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbe50261-bbb7-4d80-b035-f70abcd81033",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Basic Inspection\n",
    "print(f\"Rows: {df.count()}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "df.printSchema()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1d2f403-17db-4d41-be51-8dff69efddea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the count of nulls for each column\n",
    "# We iterate through columns to build an aggregation expression\n",
    "from pyspark.sql import functions as F\n",
    "null_counts = {\n",
    "    c: df.filter(F.col(c).isNull()).count()\n",
    "    for c in df.columns\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aa5cfdd-729c-4bce-879b-fb8f982ad062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate percentage of nulls\n",
    "total_rows =df.count()\n",
    "null_ratio = {k: v/total_rows for k , v in null_counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e97955b-952e-4f8c-9832-2e44ea9b8009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter columns that are mostly empty (e.g., > 50% null)\n",
    "cols_to_drop = [k for k, v in null_ratio.items() if v > 0.5]\n",
    "print(f\"Dropping columns: {cols_to_drop}\")\n",
    "print(cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9156e36-afbf-480f-9e45-8bc475f225ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Drop them\n",
    "df_cleaned = df.drop(*cols_to_drop)\n",
    "print(f\"new columns count: {len(df_cleaned.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df1f0cba-5d65-4bd0-a7c1-a1d72bb33dab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col,trim\n",
    "# 1. Fix 'int_rate': Remove '%' and cast to Double\n",
    "df_cleaned = df_cleaned.withColumn(\"int_rate_cleaned\",regexp_replace(col(\"int_rate\"),\"%\",\"\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dd96723-f5b2-4020-ac7a-760efc1230c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Fix 'term': Remove ' months' and cast to Integer\n",
    "# We also use trim() because 'term' often has leading spaces like \" 36 months\"\n",
    "df_cleaned = df_cleaned.withColumn(\"term_cleaned\", regexp_replace(trim(col(\"term\")),\"months\",\"\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64eed5ba-ed69-4323-a72b-e49c66a132fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned.select(\"int_rate\",\"int_rate_cleaned\",\"term\",\"term_cleaned\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "272a5161-5cdf-4244-9552-01ba1ca012da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check distinct values in loan_status before filtering\n",
    "print(\"Before filtering\")\n",
    "df_cleaned.select(\"loan_status\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90fd1335-1bde-4fdf-b0c5-dc4914a0931b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter to keep only 'Fully Paid' and 'Charged Off'\n",
    "# Note: Spark is case-sensitive, so ensure strings match exactly\n",
    "df_final = df_cleaned.filter(col(\"loan_status\").isin(\"Charged Off\", \"Fully Paid\"))\n",
    "\n",
    "#verify count after filtering\n",
    "print(f\"Final row count after filtering: {df_final.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "247571c7-9834-4d74-a47d-a8e24c5f707c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Create a new column 'loan_status_num'\n",
    "df_final = df_final.withColumn(\"loan_satus_num\", when(col(\"loan_status\") == \"Charged Off\", 1).otherwise(0))\n",
    "df_final.select(\"loan_status\", \"loan_satus_num\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60235d46-edbb-4f61-8c7f-0011bc2e5ffb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, count, round\n",
    "grade_analysis = df_final.groupBy(\"grade\") \\\n",
    "    .agg(\n",
    "        count(\"id\").alias(\"Total_loans\"), \n",
    "        round(avg(\"loan_satus_num\")*100,2).alias(\"Default_rate_percent\")\n",
    "    ) \\\n",
    "    .orderBy(\"grade\")\n",
    "grade_analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "414279c0-7d2b-438d-b0a0-798e09fd418f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group by 'term_cleaned' with correct column name\n",
    "term_analysis = df_final.groupBy(\"term_cleaned\").agg(\n",
    "    count(\"id\").alias(\"total_loans\"),\n",
    "    round(avg(\"loan_satus_num\") * 100, 2).alias(\"default_rate_percent\")\n",
    ").orderBy(\"term_cleaned\")\n",
    "\n",
    "display(term_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ff7d431-59d8-48ea-b374-8cccc5c5c894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group by 'purpose' and sort by highest default risk\n",
    "purpose_analysis = df_final.groupBy(\"purpose\").agg(\n",
    "    count(\"id\").alias(\"total_loans\"),\n",
    "    round(avg(\"loan_satus_num\") * 100, 2).alias(\"default_rate_percent\")\n",
    ").orderBy(\"default_rate_percent\", ascending=False)\n",
    "\n",
    "display(purpose_analysis)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4cde12d-61fd-4466-95ce-11c6b3d2ad84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Convert the small aggregated Spark DataFrame to Pandas\n",
    "pdf = grade_analysis.toPandas()\n",
    "\n",
    "#plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=\"grade\",y=\"Default_rate_percent\", data=pdf)\n",
    "plt.title(\"Default rate by Loan Grade\", fontsize = 16)\n",
    "plt.ylabel(\"Default Rate %\")\n",
    "plt.xlabel(\"Grade\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26194f28-7a6a-492f-9d44-7868b5556be3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LendingClubDataAnalysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
